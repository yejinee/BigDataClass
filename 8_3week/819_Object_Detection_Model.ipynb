{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"[문제]Chapter 3_Object Detection 모델 구현하기.ipynb의 사본","provenance":[],"collapsed_sections":["eTnghmuFBrEQ"],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyPuHLIT9rmsHKUtfTr1gRBO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 주제: Object Detection Model 구현하기"],"metadata":{"id":"vCa4_7v8fnkY"}},{"cell_type":"markdown","source":["## 데이터 소개\r\n","    - MS-COCO 2017 dataset을 사용\r\n","    - tfrecord 형태로 저장되어 있음\r\n","      \r\n","  ### 관련 data의 내용\r\n","    1. image\r\n","      - 각 image의 pixel 값(3차원 tensor)\r\n","      - image/filename: image file의 이름\r\n","      - image/id: image file의 id(file명에서 확장자 제외한 부분)      \r\n","      \r\n","    2. objects\r\n","      - objects/area: 각 bounding box들의 면적 \r\n","      - objects/id: 각 bounding box의 id\r\n","      - objects/bbox: 각 bounding box의 좌표(ymax, xmax, ymin, xmin)\r\n","      - objects/label: 각 bounding box에 대한 classification 정답\r\n","\r\n","- 원본 데이터 출처: https://cocodataset.org/#home\r\n","- data format 참고: https://cocodataset.org/#format-data\r\n","\r\n","## 문제 소개\r\n","    - 1-stage object detection model인 RetinaNet을 직접 만들고 학습\r\n","\r\n","## RetinaNet\r\n","    - https://arxiv.org/abs/1708.02002\r\n","    \r\n","\r\n","## 최종 목표    \r\n","    - Object Detection Model 구현"],"metadata":{"id":"eTnghmuFBrEQ"}},{"cell_type":"markdown","source":["## Step 1. Data 다운로드 및 확인"],"metadata":{"id":"6WdJtwIxbcO8"}},{"cell_type":"code","execution_count":null,"source":["## library import \r\n","import os\r\n","import re\r\n","import zipfile\r\n","\r\n","import numpy as np\r\n","import tensorflow as tf\r\n","from tensorflow import keras\r\n","\r\n","import matplotlib.pyplot as plt\r\n","import tensorflow_datasets as tfds\r\n","\r\n","import gdown"],"outputs":[],"metadata":{"id":"CvqJ-wnC9LZq"}},{"cell_type":"code","execution_count":null,"source":["## Hyper parameter 및 기타 설정\r\n","num_classes = 80\r\n","batch_size = 2\r\n","\r\n","learning_rates = [2.5e-06, 0.000625, 0.00125, 0.0025, 0.00025, 2.5e-05]\r\n","learning_rate_boundaries = [125, 250, 500, 240000, 360000]\r\n","learning_rate_fn = tf.optimizers.schedules.PiecewiseConstantDecay(\r\n","    boundaries=learning_rate_boundaries, values=learning_rates\r\n",")\r\n","\r\n","## ckpt 저장할 directory\r\n","model_dir = \"retinanet\""],"outputs":[],"metadata":{"id":"2FsaeppHcDsw"}},{"cell_type":"markdown","source":["### 문제 1. Data 불러오기\n","\n","    - data는 아래 url(google drive)에 저장되어 있습니다.(zip 파일)\n","    - gdown library를 이용하여 data를 다운받고, zip파일 압축을 풀어줍니다.\n","    - 압축 파일은 data라는 이름의 directory에 풀도록 합니다."],"metadata":{"id":"vGwb0M49Fi8w"}},{"cell_type":"code","execution_count":null,"source":["url = \"https://drive.google.com/uc?id=1vIHxg4fLsK7Vn1NnZ_toK4akqpT5ZoKM\""],"outputs":[],"metadata":{"id":"mgENz8HQxTD0"}},{"cell_type":"code","execution_count":null,"source":["## data download 받고(gdown.download 사용) 압축 풀기\r\n","##### CODE HERE #####"],"outputs":[],"metadata":{"id":"YICXwtev616c"}},{"cell_type":"markdown","source":["    - 위에서 다운받은 파일은 실제 MS-COCO data의 일부만을 포함하고 있습니다.\n","    - 아래의 cell을 실행하면 이 data를 이용하여 dataset을 만들 수 있습니다. \n","    - MS-COCO data 전체가 필요한 경우 아래 data_dir='data' 부분을 data_dir=None 으로 변경하면 됩니다.\n"],"metadata":{"id":"03nOhKLw63-a"}},{"cell_type":"code","execution_count":null,"source":["(train_dataset, val_dataset), dataset_info = tfds.load(\r\n","    \"coco/2017\", split=[\"train\", \"validation\"], with_info=True, data_dir='data'\r\n",")"],"outputs":[],"metadata":{"id":"2wGbCLUAGNSS"}},{"cell_type":"markdown","source":["    - dataset_info를 통해서 data에 관한 어떤 정보들이 있는지 확인해봅시다."],"metadata":{"id":"qhb3IOgb7ZtL"}},{"cell_type":"code","execution_count":null,"source":["dataset_info"],"outputs":[],"metadata":{"id":"M7SQC6bUOD2n"}},{"cell_type":"markdown","source":["    - tfds.show_examples를 활용하여 data에 포함된 image를 출력해봅시다."],"metadata":{"id":"oSISq8aI7tc5"}},{"cell_type":"code","execution_count":null,"source":["tfds.show_examples(train_dataset, dataset_info)"],"outputs":[],"metadata":{"id":"EXRH5t_KNWL3"}},{"cell_type":"markdown","source":["### 문제 2. Dataset 내부 item 확인\n","    - train_dataset에서 data를 하나 꺼내서 내부 item을 print 문으로 확인해봅시다."],"metadata":{"id":"xcXVrsuE70dy"}},{"cell_type":"code","execution_count":null,"source":["##### CODE HERE #####"],"outputs":[],"metadata":{"id":"L7BHYq6p7aYJ"}},{"cell_type":"markdown","source":["### 문제 3. Data 직접 확인하기\n","    - 문제 2와 같이 dataset에서 1개의 data를 가져온 후,\n","      image와 bounding box를 화면에 함께 출력해봅시다. \n","    - data에 저장된 bounding box의 좌표는 box의 양쪽 모서리의 좌표이며, \n","      (ymin, xmin, ymax, xmax)의 순서로 되어 있습니다.    "],"metadata":{"id":"cHxdSRUF81cS"}},{"cell_type":"code","execution_count":null,"source":["for data in train_dataset.take(1):\r\n","  image = np.array(data['image'], dtype=np.uint8)\r\n","  plt.figure(figsize=(8,8))\r\n","  plt.axis('off')\r\n","  plt.imshow(image)\r\n","  ax = plt.gca()\r\n","  boxes = data['objects']['bbox']\r\n","\r\n","  ##### CODE HERE #####\r\n","  \r\n","  plt.show()"],"outputs":[],"metadata":{"id":"VY7ewiJF-9TV"}},{"cell_type":"markdown","source":["## Step 2. Data Augmentation\n","이번 Step에서는 data augmentation을 구현해보도록 하겠습니다."],"metadata":{"id":"HneT9jfgjlPC"}},{"cell_type":"markdown","source":["    - 행렬과 같은 tensor의 경우 행->열 순서로 되어 있기 때문에 (y좌표, x좌표) 순서로 저장되어 있습니다.\n","    - (x좌표, y좌표) 순서가 더 직관적으로 이해하기 쉬우므로 저장된 x, y좌표의 순서를 바꿔주는 함수를 만들어봅시다.\n","    - 함수의 입력은 (N, 4) shape의 bounding box 정보이며, \n","      bounding box 좌표 정보는 양쪽 모서리의 x, y 좌표값으로 되어 있다고 가정합니다."],"metadata":{"id":"eyN5GIfPFJfQ"}},{"cell_type":"code","execution_count":null,"source":["def swap_xy(boxes):\r\n","    return tf.stack([boxes[:, 1], boxes[:, 0], boxes[:, 3], boxes[:, 2]], axis=-1)"],"outputs":[],"metadata":{"id":"o7WtjHbqztZ7"}},{"cell_type":"markdown","source":["### 문제 4. Bounding box format 변경하는 함수 만들기 1\n","    - bounding box의 위치 정보에 대한 format을 변경하는 함수를 만들어 봅시다.\n","    - (xmin, ymin, xmax, ymax) 형태의 bounding box format을 (x, y, w, h)로 바꿔서 반환합니다.\n","    - 이 때 x, y는 bounding box의 center 좌표를 의미합니다.\n","    - 함수의 입력은 (..., N, 4)와 같이 2차원 이상의 tensor입니다."],"metadata":{"id":"1-06hjOzF-OH"}},{"cell_type":"code","execution_count":null,"source":["def convert_to_xywh(boxes):    \r\n","    ##### CODE HERE #####"],"outputs":[],"metadata":{"id":"cV_40s1azwcS"}},{"cell_type":"markdown","source":["### 문제 5. Bounding box format 변경하는 함수 만들기 2\n","    - bounding box의 위치 정보에 대한 format을 변경하는 함수를 만들어 봅시다.\n","    - (x, y, w, h) 형태의 bounding box format을 (xmin, ymin, xmax, ymax)로 바꿔서 반환합니다.\n","    - 이 때 x, y는 bounding box의 center 좌표를 의미합니다.\n","    - 함수의 입력은 (..., N, 4)와 같이 2차원 이상의 tensor입니다."],"metadata":{"id":"i-c9LiCHHptf"}},{"cell_type":"code","execution_count":null,"source":["def convert_to_corners(boxes):\r\n","    ##### CODE HERE #####"],"outputs":[],"metadata":{"id":"CewGC3MBzwfE"}},{"cell_type":"markdown","source":["### 문제 6. Image resizing 함수 만들기\n","    - 짧은 변을 min_side와 같게 resize 합니다\n","    - 만약에 긴 변의 길이가 max_side보다 클 경우에는 긴 변이 max_side와 같아지도록 다시 resize 합니다\n","    - image size(가로, 세로 모두)가 stride의 배수가 아닐 경우 stride의 배수가 되도록 오른쪽과 아래쪽에 0을 채웁니다(zero padding) \n","    - 함수의 입력값은 다음과 같습니다.\n","      1. image: 3차원 tensor로 이루어진 image(feature map)의 pixel 값(height, width, channel)\n","      2. min_side: resize에 사용할 짧은 변 길이\n","      3. max_side: resize에 사용할 긴 변 길이\n","      4. stride: 1번 입력인 image(feature map)의 1 pixel이 실제 원본 image에서 몇 pixel에 해당되는지\n","    - 함수의 반환값은 3가지이고 각각 다음과 같습니다.\n","      1. resize 및 padding된 image(feature map)의 pixel 값\n","      2. padding 하기 전의 image size\n","      3. padding 하기 전 image와 원본 image의 확대/축소 비율(resize 후/resize 전)\n"],"metadata":{"id":"LP-a-RIL_G5X"}},{"cell_type":"code","execution_count":null,"source":["def resize_and_pad_image(image, min_side=800.0, max_side=1333.0, stride=128.0):    \r\n","    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)    \r\n","    ratio = min_side / tf.reduce_min(image_shape)\r\n","\r\n","    ##### CODE HERE #####\r\n","    \r\n","    return image, image_shape, ratio"],"outputs":[],"metadata":{"id":"Slp75Jwb_GKF"}},{"cell_type":"markdown","source":["### 문제 7. Horizontal flip 함수 만들기\n","    - 50%의 확률로 image를 좌우반전합니다\n","    - 이 때 bounding box의 좌표도 좌우반전에 맞게 변경합니다\n","    - 입력 image는 3차원 tensor로 (height, width, channel)로 구성되어 있습니다.\n","    - bounding box의 좌표는 (xmin, ymin, xmax, ymax)로 구성되어 있고, 각각의 좌표는 0~1 사이 값으로 normalized 되어 있다고 가정합니다.\n","    - 함수의 반환값은 image의 pixel 값, bounding box의 좌표로 합니다."],"metadata":{"id":"TIcF3LCHArQ9"}},{"cell_type":"code","execution_count":null,"source":["def random_flip_horizontal(image, boxes):\r\n","    ##### CODE HERE #####\r\n","    return image, boxes"],"outputs":[],"metadata":{"id":"QA4-ihsj9Uwb"}},{"cell_type":"markdown","source":["### 문제 8. Augmentation 함수 만들기\n","    - tf.data(dataset)에 map으로 적용할 수 있도록 함수를 만듭니다.\n","    - 입력으로 dataset의 item을 받습니다\n","    - 위에서 작성한 random_flip_horizontal 함수를 먼저 적용하고, 다음으로 resize_and_pad_image 함수를 적용합니다.\n","    - 0~1 사이 값으로 normailized된 bounding box 좌표를 실제 image size에 맞게 조정합니다.\n","    - bounding box 좌표를 (xmin, ymin, xmax, ymax) 형태에서 (x, y, w, h) 형태로 변경합니다.\n","    - 함수의 반환값은 image의 pixel 값, bounding box의 좌표, class id로 합니다."],"metadata":{"id":"2iisXvZbB4YF"}},{"cell_type":"code","execution_count":null,"source":["def preprocess_data(sample):\r\n","    image = sample[\"image\"]\r\n","    bbox = swap_xy(sample[\"objects\"][\"bbox\"])\r\n","    class_id = tf.cast(sample[\"objects\"][\"label\"], dtype=tf.int32)\r\n","\r\n","    ##### CODE HERE #####\r\n","    \r\n","    return image, bbox, class_id"],"outputs":[],"metadata":{"id":"608byiMxENdV"}},{"cell_type":"markdown","source":["## Step 3. Anchor Box 정보 만들기\r\n","AnchorBox class를 만들고, 모든 anchor box의 (x, y, w, h) 정보 생성"],"metadata":{"id":"MVuBmsUc3_GL"}},{"cell_type":"markdown","source":["      * _compute_dims: 각 level별로 ahcnor box의 (w,h)를 계산하여 반환합니다.\n","      * _get_anchors: 각 level별로 anchor box의 (x,y,w,h)를 계산하여 반환합니다. 이 때 return shape은 (height*width*9, 4)입니다.\n","      * get_anchors: _get_anchors의 level별 anchor box정보를 모두 합쳐서 최종 결과를 반환합니다."],"metadata":{"id":"c5d3mtdQUpUO"}},{"cell_type":"markdown","source":["    - AnchorBox class를 만들고, get_anchors method를 call하면 모든 anchor box의 (x,y,w,h) 좌표를 반환하도록 합니다.    \n","    - P3-P7까지의 모든 level의 anchor box의 (x,y,w,h) 좌표가 반환되어야 합니다.\n","    - 계산의 편의를 위하여 먼저 각 level의 anchor box를 (height, width, 9, 4)의 shape을 갖는 tensor로 만듭니다. \n","      여기서 height, width는 각 level의 feature map size를 의미하며, 9는 각 level의 anchor box 갯수이며 4는 (x,y,w,h)를 말합니다.\n","    - 최종적으로 반환되는 값은 (5*height*width*9, 4)의 형태가 되며 맨 앞의 5는 level의 갯수(P3, P4, P5, P6, P7)을 의미합니다."],"metadata":{"id":"edpIGs9ET8j1"}},{"cell_type":"markdown","source":["Class 내부의 method는 다음과 같습니다"],"metadata":{"id":"UTHjUA6sVQZF"}},{"cell_type":"markdown","source":["### \\_\\_init\\_\\_ method\n","    - __init__ : anchor box 정보 계산을 위한 기본 값들 setting"],"metadata":{"id":"Ef03QGGiUaUX"}},{"cell_type":"markdown","source":["### 문제 9. _compute_dims method\n","    - _compute_dims method를 만들어봅시다.\n","    - _compute_dims는 모든 level(P3-P7)에 대하여 각 level별로 anchor box의 (w, h)를 계산하여 반환하는 역할을 합니다.\n","    - anchor_dims_all list에 해당 정보가 저장되며 list의 원소는 각각 (1,1,9,2)의 shape을 갖게 됩니다."],"metadata":{"id":"sPiKO542Ur5t"}},{"cell_type":"markdown","source":["### 문제 10. _get_anchors method\n","    - _get_anchors method를 만들어봅시다.\n","    - _get_anchors method는 각 level별로 anchor box의 (x,y,w,h)를 계산하여 반환합니다.\n","    - 반환값의 shape은 (height*width*9, 4)입니다."],"metadata":{"id":"WbIjuz9rVjGE"}},{"cell_type":"markdown","source":["### 문제 11. get_anchors method\n","    - get_anchors method를 만들어봅시다.\n","    - get_anchors method는 문제 9의 _get_anchors 를 통해서 level별 anchor box의 정보를 받아 이를 모두 합쳐서 최종 결과를 반환합니다.\n","    - 반환값의 shape은 (5*height*width*9, 4)입니다."],"metadata":{"id":"n4VJ-ddeWE_e"}},{"cell_type":"code","execution_count":null,"source":["class AnchorBox:\r\n","    def __init__(self):        \r\n","        self.aspect_ratios = [0.5, 1.0, 2.0]\r\n","        self.scales = [2 ** x for x in [0, 1 / 3, 2 / 3]]\r\n","\r\n","        self._num_anchors = len(self.aspect_ratios) * len(self.scales)\r\n","        self._strides = [2 ** i for i in range(3, 8)]\r\n","        self._areas = [x ** 2 for x in [32.0, 64.0, 128.0, 256.0, 512.0]]\r\n","        self._anchor_dims = self._compute_dims()\r\n","\r\n","    def _compute_dims(self):\r\n","        anchor_dims_all = []\r\n","        for area in self._areas:\r\n","            anchor_dims = []\r\n","            for ratio in self.aspect_ratios:\r\n","                anchor_height = tf.math.sqrt(area / ratio)\r\n","\r\n","                ##### CODE HERE #####\r\n","\r\n","                for scale in self.scales:\r\n","                    anchor_dims.append(scale * dims)\r\n","            anchor_dims_all.append(tf.stack(anchor_dims, axis=-2))\r\n","        return anchor_dims_all\r\n","\r\n","    def _get_anchors(self, feature_height, feature_width, level):\r\n","        rx = tf.range(feature_width, dtype=tf.float32) + 0.5\r\n","        ry = tf.range(feature_height, dtype=tf.float32) + 0.5\r\n","\r\n","        ##### CODE HERE #####\r\n","        \r\n","        return tf.reshape(\r\n","            anchors, [feature_height * feature_width * self._num_anchors, 4]\r\n","        )\r\n","\r\n","    def get_anchors(self, image_height, image_width):\r\n","        anchors = [\r\n","            ##### CODE HERE #####\r\n","            for i in range(3, 8)\r\n","        ]\r\n","        return tf.concat(anchors, axis=0)"],"outputs":[],"metadata":{"id":"96lom4LRAuFA"}},{"cell_type":"markdown","source":["## Step 4. Label Encoding\n","anchor box 정보와 정답(groundtruth) box 정보, class id를 이용하여 detection에서 사용할 label을 만들어봅시다."],"metadata":{"id":"5svKDLTNUo1m"}},{"cell_type":"markdown","source":["### 문제 12. IOU 계산 함수 만들기\n","\n","    - 이 함수는 2개의 bounding box 그룹들(boxes1, boxes2) 간에 iou를 계산합니다\n","    - 모든 anchor box와 gound truth box 간의 iou를 계산하여,\n","      anchor box들을 positive, negative, ignore로 구분하기 위해서 사용합니다\n","    - 각 bounding box의 좌표 정보는 (x,y,w,h) 형태로 입력받습니다.\n","    - boxes1의 shape은 (N, 4)이고 boxes2의 shape은 (M, 4)라고 할 때, 출력은 (N, M) shape의 tensor가 됩니다."],"metadata":{"id":"buP69TdpWpPd"}},{"cell_type":"code","execution_count":null,"source":["def compute_iou(boxes1, boxes2):\r\n","    boxes1_corners = convert_to_corners(boxes1)\r\n","    boxes2_corners = convert_to_corners(boxes2)\r\n","\r\n","    ##### CODE HERE #####\r\n","    \r\n","    return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0)"],"outputs":[],"metadata":{"id":"IBOaK35TNMkp"}},{"cell_type":"markdown","source":["### LabelEncoder Class\n","이제 LabelEncoder class를 작성해보겠습니다.\n","이 class에서는 앞에서 생성한 anchor box 정보와 groundtruth box 정보, class id를 이용하여 실제 학습에 사용할 label을 생성해줍니다.\n","해당 class의 method는 다음과 같습니다.\n","\n","### \\_\\_init\\_\\_ method\n","\n","    - AnchorBox class instance 생성\n","    - box normalize를 위한 variance\n","    \n"],"metadata":{"id":"DdCyK1gWZy3-"}},{"cell_type":"markdown","source":["### 문제 13. _match_anchor_boxes method\n","    - IOU를 기반으로 gt box를 anchor box에 matching하는 역할을 합니다. \n","    - M개의 anchor box와 N개의 gt box에 대하여 계산된 MxN의 iou matrix를 이용하여 각 행에서 IOU의 최대값을 찾습니다.\n","    - 이 때 최대 IOU에 해당되는 index도 저장합니다.(matched_gt_index)\n","    - 최대 IOU 값이 match_iou 이상이면 positive, ignore_iou 미만이면 negative, 나머지는 ignore로 처리하여\n","      각 anchor box마다 positive, negative, ignore 여부를 알 수 있는 postive_mask, negative mask, ignore_mask를 생성합니다.\n","    - 입력값은 anchor box의 (x,y,w,h)정보, gt box의 (x,y,w,h)정보, match_iou, ignore_iou 입니다.\n","    - 반환값은 matched_gt_index, positive_mask, ignore_mask 입니다."],"metadata":{"id":"eHjAsAS5BwwT"}},{"cell_type":"markdown","source":["### 문제 14. _compute_box_target method\n","    - 각 anchor box에 대한 transform 값을 계산합니다.\n","    - transform은 gt box와 같아지기 위해서 anchor box의 x,y,w,h를 얼마나 변형해야 하는가에 대한 값입니다.\n","    - 입력값으로 각 anchor box와 그에 대한 target 값을 받습니다.\n","    - 반환값은 각 anchor box의 transform label입니다. "],"metadata":{"id":"MTNkKJnSFKoB"}},{"cell_type":"markdown","source":["### 문제 15. _encode_sample method\n","    - 각 anchor box에 대한 bbox와 class의 target 값을 계산합니다.\n","    - _match_anchor_boxes method에서 계산한 최대 IOU의 index를 활용하여 해당되는 gt box의 정보를 찾아서 matching 해줍니다.\n","    - matching돤 gt box의 x,y,w,h와 _compute_box_target method를 이용하여, box regression에 대한 label을 생성합니다.\n","    - matching된 gt box의 class id와 positive_mask, ignore_mask를 이용하여 class에 대한 label을 생성합니다.\n","    - 이 때 positive인 경우는 gt의 class id를 사용하고, negative인 경우는 -1로 ignore인 경우는 -2로 labeling합니다.\n","    - 입력값으로 image shape(batch, height, width, channel), gt box의 (x,y,w,h), gt box의 class id를 받습니다.\n","    - 최종적으로 box target과 class target을 하나로 concat하여 반환합니다. \n"],"metadata":{"id":"bqAAvVKQKFqZ"}},{"cell_type":"markdown","source":["### 문제 16. encode_batch method\n","    - tf.data(dataset)에 map으로 적용할 수 있도록 합니다.\n","    - batch 단위로 data를 받아서, 각 image마다 위에서 작성한 method들을 활용하여 label을 생성하고,\n","      그 label들을 다시 batch 단위로 묶어서 반환합니다.\n","    - 입력값으로 batch 단위의 image data, gt box의 (x,y,w,h), gt box의 class id를 받습니다.\n","    - 반환값은 batch 단위의 image data(입력값 그대로)와, 생성한 label입니다."],"metadata":{"id":"GasBMcQ8MEbG"}},{"cell_type":"code","execution_count":null,"source":["class LabelEncoder:\r\n","    def __init__(self):\r\n","        self._anchor_box = AnchorBox()\r\n","        self._box_variance = tf.convert_to_tensor(\r\n","            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\r\n","        )\r\n","\r\n","    def _match_anchor_boxes(self, anchor_boxes, gt_boxes, match_iou=0.5, ignore_iou=0.4):        \r\n","        iou_matrix = compute_iou(anchor_boxes, gt_boxes)\r\n","\r\n","        ##### CODE HERE #####\r\n","\r\n","        return (\r\n","            matched_gt_idx,\r\n","            tf.cast(positive_mask, dtype=tf.float32),\r\n","            tf.cast(ignore_mask, dtype=tf.float32),\r\n","        )\r\n","\r\n","    def _compute_box_target(self, anchor_boxes, matched_gt_boxes):        \r\n","        box_target = tf.concat(\r\n","            [\r\n","               ##### CODE HERE #####\r\n","            ],\r\n","            axis=-1,\r\n","        )\r\n","        box_target = box_target / self._box_variance\r\n","        return box_target\r\n","\r\n","    def _encode_sample(self, image_shape, gt_boxes, cls_ids):        \r\n","        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\r\n","        cls_ids = tf.cast(cls_ids, dtype=tf.float32)\r\n","        matched_gt_idx, positive_mask, ignore_mask = self._match_anchor_boxes(\r\n","            anchor_boxes, gt_boxes\r\n","        )\r\n","\r\n","        ##### CODE HERE #####\r\n","\r\n","        label = tf.concat([box_target, cls_target], axis=-1)\r\n","        return label\r\n","\r\n","    def encode_batch(self, batch_images, gt_boxes, cls_ids):       \r\n","        images_shape = tf.shape(batch_images)\r\n","        batch_size = images_shape[0]\r\n","\r\n","        labels = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\r\n","        for i in range(batch_size):\r\n","\r\n","            ##### CODE HERE #####\r\n","            \r\n","        batch_images = tf.keras.applications.resnet.preprocess_input(batch_images)\r\n","        return batch_images, labels.stack()"],"outputs":[],"metadata":{"id":"1iwYSOW7NMnj"}},{"cell_type":"markdown","source":["## Step 5. Dataset 만들기\n","앞에서 작성한 함수와 class를 활용하여 model에 data를 공급하기 위한 dataset을 만들어봅시다.\n"],"metadata":{"id":"_1IoMQFRSmkH"}},{"cell_type":"markdown","source":["### 문제 17. Train/Validation dataset 만들기\n","    - 위에서 생성한 train_dataset, validation_dataset과 그동안 작성한 함수와 class들을 활용하여,\n","      train/validation dataset을 만듭니다.\n","    - 다음과 같은 순서로 적용합니다.\n","      1. preprocess_data 함수 적용\n","      2. dataset shuffle\n","      3. padded_batch를 이용하여 batch로 묶음\n","        - 이 때, padding 값은 image, box 좌표, class id에 대하여 각각 0.0, 1e-8, -1로 합니다.\n","      4. LabelEncoder의 encode_batch 함수 적용\n","      5. prefetch 적용\n","    - validation_dataset에는 shuffle만 빼고 동일하게 적용합니다.\n","\n"],"metadata":{"id":"aId2ac5IZLA2"}},{"cell_type":"code","execution_count":null,"source":["label_encoder = LabelEncoder()\n","\n","autotune = tf.data.AUTOTUNE\n","\n","##### CODE HERE #####\n","\n","train_dataset = train_dataset.apply(tf.data.experimental.ignore_errors())\n","train_dataset = train_dataset.prefetch(autotune)\n","\n","\n","##### CODE HERE #####\n","\n","val_dataset = val_dataset.apply(tf.data.experimental.ignore_errors())\n","val_dataset = val_dataset.prefetch(autotune)"],"outputs":[],"metadata":{"id":"o0SZL_vUNMq_"}},{"cell_type":"markdown","source":["## Step 6. RetinaNet Model 만들기\n","Data가 준비되었으므로, RetinaNet model을 만들어보도록 하겠습니다."],"metadata":{"id":"eka4tTWbcgOO"}},{"cell_type":"markdown","source":["### 문제 18. get_backbone 함수 만들기\n","    - backbone으로 ResNet50을 사용합니다.\n","    - get_backbone 함수는 ResNet50의 C3, C4, C5에 해당하는 feature map을 output으로 하는 model을 반환합니다.\n"],"metadata":{"id":"niV-FXmvcrrR"}},{"cell_type":"markdown","source":["먼저 ResNet50을 가져와서 C3, C4, C5에 해당하는 layer의 이름을 확인합니다."],"metadata":{"id":"rBzL5ebweUZh"}},{"cell_type":"code","execution_count":null,"source":["backbone = keras.applications.ResNet50(include_top=False, input_shape=[224, 224, 3])"],"outputs":[],"metadata":{"id":"2_RqC-0Mcghi"}},{"cell_type":"code","execution_count":null,"source":["backbone.summary()"],"outputs":[],"metadata":{"id":"mw0NuGgPcgke"}},{"cell_type":"markdown","source":["확인된 layer 이름을 활용하여 get_backbone 함수를 작성합니다."],"metadata":{"id":"Xk4mpjqXe3Ba"}},{"cell_type":"code","execution_count":null,"source":["def get_backbone():    \n","    backbone = keras.applications.ResNet50(\n","        include_top=False, input_shape=[None, None, 3]\n","    )\n","\n","    ##### CODE HERE #####\n","    \n","    return keras.Model(\n","        inputs=[backbone.inputs], outputs=[c3_output, c4_output, c5_output]\n","    )"],"outputs":[],"metadata":{"id":"VjQE-1ZJcgnQ"}},{"cell_type":"markdown","source":["### 문제 19. Feature Pyramid Network 만들기\n","keras custom layer를 활용하여 FPN을 만들어봅시다.\n","\n","    - input으로 batch 단위의 image를 받고 output으로 P3-P7을 반환합니다.\n","    - RetinaNet과 FPN 논문의 Feature Pyramid Network 부분을 참고하여 작성합니다."],"metadata":{"id":"gouQxDVCfHFL"}},{"cell_type":"code","execution_count":null,"source":["class FeaturePyramid(keras.layers.Layer):\n","    def __init__(self, **kwargs):\n","        super(FeaturePyramid, self).__init__(name=\"FeaturePyramid\", **kwargs)\n","        self.backbone = get_backbone()\n","        self.conv_c3_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n","        self.conv_c4_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n","        self.conv_c5_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n","        self.conv_c3_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n","        self.conv_c4_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n","        self.conv_c5_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n","        self.conv_c6_3x3 = keras.layers.Conv2D(256, 3, 2, \"same\")\n","        self.conv_c7_3x3 = keras.layers.Conv2D(256, 3, 2, \"same\")\n","        self.upsample_2x = keras.layers.UpSampling2D(2)\n","\n","    def call(self, images, training=False):\n","\n","        ##### CODE HERE #####\n","        \n","        return p3_output, p4_output, p5_output, p6_output, p7_output"],"outputs":[],"metadata":{"id":"nFAqabf8HkbN"}},{"cell_type":"markdown","source":["### 문제 20. Class/Box subnet 만들기\n","classification과 box regression을 위한 subnet(head)를 만들어봅시다.\n","\n","    - classification과 box regression의 subnet 구조가 동일하므로 공통으로 사용할 수 있는 함수 형태로 작성합니다.\n","    - 이 함수는 입력값으로 마지막 layer의 convolution filter 수와 bias initializaer를 받습니다.\n","    - channel이 256인 feature map을 입력으로 classification 혹은 box regression의 결과를 뽑아주는 모델을 반환합니다. "],"metadata":{"id":"hV7IFjwglCYh"}},{"cell_type":"code","execution_count":null,"source":["def build_head(output_filters, bias_init):\n","    head = keras.Sequential([keras.Input(shape=[None, None, 256])])\n","    kernel_init = tf.initializers.RandomNormal(0.0, 0.01)\n","\n","    ##### CODE HERE #####\n","    \n","    head.add(\n","        keras.layers.Conv2D(\n","            output_filters,\n","            3,\n","            1,\n","            padding=\"same\",\n","            kernel_initializer=kernel_init,\n","            bias_initializer=bias_init,\n","        )\n","    )\n","    return head"],"outputs":[],"metadata":{"id":"5t66npUEOb4d"}},{"cell_type":"markdown","source":["### 문제 21. RetinaNet model 만들기\n","위에서 작성한 layer 및 함수를 이용하여 RetinaNet model을 만들어봅시다.\n","\n","    - keras subclassing model을 이용하여 작성합니다.\n","    - 입력으로 image를 받고, 출력으로 모든 anchor box의 regression과 classification 예측값을 모아서 반환합니다.\n","    - classification subnet에는 RetinaNet 논문에 나온 bias initialzation을 적용합니다.\n","    - output shape은 (batch size, 전체 anchor box의 갯수, num_classes+4[84]) 입니다."],"metadata":{"id":"-3lbYVRem5zY"}},{"cell_type":"code","execution_count":null,"source":["class RetinaNet(keras.Model):\n","    def __init__(self, num_classes, **kwargs):\n","        super(RetinaNet, self).__init__(name=\"RetinaNet\", **kwargs)\n","        self.fpn = FeaturePyramid()\n","        self.num_classes = num_classes\n","\n","        prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n","        self.cls_head = build_head(9 * num_classes, prior_probability)\n","        self.box_head = build_head(9 * 4, \"zeros\")\n","\n","    def call(self, image, training=False):\n","        features = self.fpn(image, training=training)\n","        N = tf.shape(image)[0]\n","        cls_outputs = []\n","        box_outputs = []\n","\n","        ##### CODE HERE #####\n","        \n","        cls_outputs = tf.concat(cls_outputs, axis=1)\n","        box_outputs = tf.concat(box_outputs, axis=1)\n","        return tf.concat([box_outputs, cls_outputs], axis=-1)"],"outputs":[],"metadata":{"id":"P3Qc8EQHGnLh"}},{"cell_type":"markdown","source":["## Step 7. Model 학습하기\n","Loss function을 정의하고 RetinaNet model을 학습해봅시다."],"metadata":{"id":"vKMgFVJbl_n0"}},{"cell_type":"markdown","source":["### 문제 22. Smooth L1 loss\n","    - box regression을 위한 smooth l1 loss를 만들어봅시다.\n","    - tf.losses.Loss class의 subclass로 custom loss를 만듭니다.\n","    - l1, l2 loss가 변하는 지점의 값은 delta 값으로 __init__ method에서 입력받습니다."],"metadata":{"id":"AtBCGTbKmM6f"}},{"cell_type":"code","execution_count":null,"source":["class RetinaNetBoxLoss(tf.losses.Loss):\n","    \"\"\"Implements Smooth L1 loss\"\"\"\n","\n","    def __init__(self, delta):\n","        super(RetinaNetBoxLoss, self).__init__(\n","            reduction=\"none\", name=\"RetinaNetBoxLoss\"\n","        )\n","        self._delta = delta\n","\n","    def call(self, y_true, y_pred):\n","        difference = y_true - y_pred\n","\n","        ##### CODE HERE #####\n","        \n","        return tf.reduce_sum(loss, axis=-1)"],"outputs":[],"metadata":{"id":"hRL4nXbUpXfJ"}},{"cell_type":"markdown","source":["### 문제 23. Focal loss\n","    - classification을 위한 focal loss를 만들어봅시다.\n","    - smooth l1 loss와 마찬가지로 subclassing을 이용하고, 논문의 수식을 참고하여 구현합니다.\n","    - alpha, gamma 값은 __init__ method에서 설정합니다."],"metadata":{"id":"9b8qWFAwnnzC"}},{"cell_type":"code","execution_count":null,"source":["class RetinaNetClassificationLoss(tf.losses.Loss):\n","    \"\"\"Implements Focal loss\"\"\"\n","\n","    def __init__(self, alpha, gamma):\n","        super(RetinaNetClassificationLoss, self).__init__(\n","            reduction=\"none\", name=\"RetinaNetClassificationLoss\"\n","        )\n","        self._alpha = alpha\n","        self._gamma = gamma\n","\n","    def call(self, y_true, y_pred):\n","        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n","            labels=y_true, logits=y_pred\n","        )\n","        probs = tf.nn.sigmoid(y_pred)\n","\n","        ##### CODE HERE #####\n","        \n","        return tf.reduce_sum(loss, axis=-1)"],"outputs":[],"metadata":{"id":"KPLQtAuanbL0"}},{"cell_type":"markdown","source":["### 문제 24. RetinaNet loss\n","    - smooth l1 loss와 focal loss를 합쳐서 RetinaNet loss를 만들어봅시다.\n","    - classification loss는 ignore의 경우를 제외합니다.\n","    - box regression loss는 positive에 대해서만 계산합니다.\n","    - 각 loss는 positive sample의 갯수로 normalize합니다.\n","    - 최종 loss는 classification loss와 regression loss를 더해서 계산합니다.\n"],"metadata":{"id":"iT2PxGIIoVAw"}},{"cell_type":"code","execution_count":null,"source":["class RetinaNetLoss(tf.losses.Loss):\n","    \"\"\"Wrapper to combine both the losses\"\"\"\n","\n","    def __init__(self, num_classes=80, alpha=0.25, gamma=2.0, delta=1.0):\n","        super(RetinaNetLoss, self).__init__(reduction=\"auto\", name=\"RetinaNetLoss\")\n","        self._cls_loss = RetinaNetClassificationLoss(alpha, gamma)\n","        self._box_loss = RetinaNetBoxLoss(delta)\n","        self._num_classes = num_classes\n","\n","    def call(self, y_true, y_pred):\n","        y_pred = tf.cast(y_pred, dtype=tf.float32)\n","        box_labels = y_true[:, :, :4]\n","        box_predictions = y_pred[:, :, :4]\n","        cls_labels = tf.one_hot(\n","            tf.cast(y_true[:, :, 4], dtype=tf.int32),\n","            depth=self._num_classes,\n","            dtype=tf.float32,\n","        )\n","        cls_predictions = y_pred[:, :, 4:]\n","        positive_mask = tf.cast(tf.greater(y_true[:, :, 4], -1.0), dtype=tf.float32)\n","        ignore_mask = tf.cast(tf.equal(y_true[:, :, 4], -2.0), dtype=tf.float32)\n","\n","        ##### CODE HERE #####\n","        \n","        loss = cls_loss + box_loss\n","        return loss"],"outputs":[],"metadata":{"id":"tlcx0P0woUHM"}},{"cell_type":"markdown","source":["### 문제 25. Model compile\n","    - RetinaNet model을 만들고, RetenaNet loss를 이용하여 model을 compile 해봅시다.\n","    - optimizer는 momentum SGD를 사용하고, momentum 값은 0.9를 사용합니다.\n","    - learning rate schedule은 위에서 hyperparameter 설정부분에서 만들어 둔 것을 사용합니다."],"metadata":{"id":"m_jl1v5L5vAp"}},{"cell_type":"code","execution_count":null,"source":["##### CODE HERE #####\n","\n","model.compile(loss=loss_fn, optimizer=optimizer)"],"outputs":[],"metadata":{"id":"VKv6IPOC5dLa"}},{"cell_type":"markdown","source":["### Checkpoint callback\n","    - 다음과 같이 model의 weight를 저장하기 위한 checkpoint callback을 만듭니다."],"metadata":{"id":"HHoULfsQ6d43"}},{"cell_type":"code","execution_count":null,"source":["callbacks_list = [\n","    tf.keras.callbacks.ModelCheckpoint(\n","        filepath=os.path.join(model_dir, \"weights\" + \"_epoch_{epoch}\"),\n","        monitor=\"loss\",\n","        save_best_only=False,\n","        save_weights_only=True,\n","        verbose=1,\n","    )\n","]"],"outputs":[],"metadata":{"id":"NH5ia2wp6XI4"}},{"cell_type":"markdown","source":["### Model 학습하기\n","    - model.fit으로 model을 학습합니다.\n","    - MS-COCO 전체 data를 이용하여 학습하는 경우에는 아래 주석 처리된 부분의 주석을 지우고,\n","      epochs = 1 부분을 삭제,\n","      train_dataset.take(100), val_dataset.take(50)의 .take 부분을 삭제하면 학습할 수 있습니다."],"metadata":{"id":"ljhAPyHi6w_L"}},{"cell_type":"code","execution_count":null,"source":["# train_steps_per_epoch = dataset_info.splits[\"train\"].num_examples // batch_size\n","# val_steps_per_epoch = \\\n","#     dataset_info.splits[\"validation\"].num_examples // batch_size\n","\n","# train_steps = 4 * 100000\n","# epochs = train_steps // train_steps_per_epoch\n","\n","epochs = 1\n","\n","model.fit(\n","    train_dataset.take(100),\n","    validation_data=val_dataset.take(50),\n","    epochs=epochs,\n","    callbacks=callbacks_list,\n","    verbose=1,\n",")"],"outputs":[],"metadata":{"id":"_AYgR9ux6pHP"}},{"cell_type":"markdown","source":["## Step 8. 학습된 Model로 결과 확인하기\n","학습된 model의 weight를 불러와서 validation set에 대하여 detection 결과를 확인해봅시다."],"metadata":{"id":"at9VOq-jOUYv"}},{"cell_type":"markdown","source":["### 학습된 Weights Loading\n","    - 미리 학습해둔 weights를 아래와 같이 다운받고, model에 load 합니다."],"metadata":{"id":"Q1L6hbQOPQLI"}},{"cell_type":"code","execution_count":null,"source":["ckpt_url = \"https://drive.google.com/uc?id=19snoNsuyeLPxkj9Is1cMmY-JviTydZim\"\n","\n","gdown.download(ckpt_url, 'ckpt.zip', quiet=False)\n","\n","with zipfile.ZipFile(\"ckpt.zip\", \"r\") as z_fp:\n","    z_fp.extractall(\"./ckpt\")"],"outputs":[],"metadata":{"id":"bX6iEshc-ngV"}},{"cell_type":"code","execution_count":null,"source":["# Change this to `model_dir` when not using the downloaded weights\n","weights_dir = \"ckpt\"\n","\n","latest_checkpoint = tf.train.latest_checkpoint(weights_dir)\n","model.load_weights(latest_checkpoint)"],"outputs":[],"metadata":{"id":"HXQ6VPLtMx1y"}},{"cell_type":"markdown","source":["### Model 예측 결과를 Decoding하기\n","RetinaNet은 anchor box 내의 물체에 대한 class별 확률과 anchor box를 얼마나 변형시킬지의 대한 값을 예측합니다.\n","\n","이를 이용하여 bounding box를 transform하고, class에 대한 예측을 계산합니다.\n","\n","이를 위하여 DecodePredictions 라는 custom layer를 생성하고, model의 prediction에 이 layer를 통과시켜서 최종 예측값을 얻는 형태로 구현합니다.\n","\n","DecodePredictions의 method는 다음과 같습니다."],"metadata":{"id":"I6hddicGPagj"}},{"cell_type":"markdown","source":["### \\_\\_init\\_\\_ method\n","     - anchor box를 생성하고, nms를 위한 thresould 값들, 그리고 maximum detection의 수를 설정합니다."],"metadata":{"id":"Ak7-I0KEXJgV"}},{"cell_type":"markdown","source":["### 문제 26. _decode_box_predictions method\n","    - anchor box의 (x,y,w,h)와 model의 box regression 예측값을 이용하여 transform된 box의 위치를 계산합니다.\n","    - 화면에 출력하기 쉽게 하기 위하여 이 값을 (xmin, ymin, xmax, ymax)로 변환하여 반환합니다"],"metadata":{"id":"_x1Y6v2oW2v8"}},{"cell_type":"markdown","source":["### 문제 27. call method\n","    - box regression 값과 classification 예측값을 받아서, box regreesion 값은 _decode_box_predictions에 넣어주고 classification은 sigmoid를 이용하여 확률값으로 변경해줍니다.\n","    - tf.image.combined_non_max_suppresion을 이용하여 nms를 수행하고 결과를 반환합니다."],"metadata":{"id":"Nc79lbuuX-Mw"}},{"cell_type":"code","execution_count":null,"source":["class DecodePredictions(tf.keras.layers.Layer):\n","    def __init__(\n","        self,\n","        num_classes=80,\n","        confidence_threshold=0.05,\n","        nms_iou_threshold=0.5,\n","        max_detections_per_class=100,\n","        max_detections=100,\n","        box_variance=[0.1, 0.1, 0.2, 0.2],\n","        **kwargs\n","    ):\n","        super(DecodePredictions, self).__init__(**kwargs)\n","        self.num_classes = num_classes\n","        self.confidence_threshold = confidence_threshold\n","        self.nms_iou_threshold = nms_iou_threshold\n","        self.max_detections_per_class = max_detections_per_class\n","        self.max_detections = max_detections\n","\n","        self._anchor_box = AnchorBox()\n","        self._box_variance = tf.convert_to_tensor(\n","            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n","        )\n","\n","    def _decode_box_predictions(self, anchor_boxes, box_predictions):\n","        boxes = box_predictions * self._box_variance\n","        boxes = tf.concat(\n","            [\n","                ##### CODE HERE #####\n","            ],\n","            axis=-1,\n","        )\n","        boxes_transformed = convert_to_corners(boxes)\n","        return boxes_transformed\n","\n","    def call(self, images, predictions):\n","        image_shape = tf.cast(tf.shape(images), dtype=tf.float32)\n","        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n","        \n","        ##### CODE HERE #####\n","\n","        return tf.image.combined_non_max_suppression(\n","            tf.expand_dims(boxes, axis=2),\n","            cls_predictions,\n","            self.max_detections_per_class,\n","            self.max_detections,\n","            self.nms_iou_threshold,\n","            self.confidence_threshold,\n","            clip_boxes=False,\n","        )"],"outputs":[],"metadata":{"id":"txeWIQdJM5B3"}},{"cell_type":"markdown","source":["### Validation data를 이용하여 결과를 화면에 출력하기\n","\n","    - image를 입력으로 하고 이를 학습된 RetinaNet을 통과시킨 뒤,\n","      위에서 작성한 DecodePredictions를 통과한 결과가 출력이 되는 model을 하나 생성합니다."],"metadata":{"id":"C97ZdBhJYwiS"}},{"cell_type":"code","execution_count":null,"source":["image = tf.keras.Input(shape=[None, None, 3], name=\"image\")\n","predictions = model(image, training=False)\n","detections = DecodePredictions(confidence_threshold=0.5)(image, predictions)\n","inference_model = tf.keras.Model(inputs=image, outputs=detections)"],"outputs":[],"metadata":{"id":"nHAaXZgiM5MD"}},{"cell_type":"markdown","source":["    - image와 bounding box, class name을 출력하기 위한 함수를 다음과 같이 작성합니다."],"metadata":{"id":"ciAlrHkNZMNd"}},{"cell_type":"code","execution_count":null,"source":["def visualize_detections(\n","    image, boxes, classes, scores, figsize=(7, 7), linewidth=1, color=[0, 0, 1]\n","):\n","    \"\"\"Visualize Detections\"\"\"\n","    image = np.array(image, dtype=np.uint8)\n","    plt.figure(figsize=figsize)\n","    plt.axis(\"off\")\n","    plt.imshow(image)\n","    ax = plt.gca()\n","    for box, _cls, score in zip(boxes, classes, scores):\n","        text = \"{}: {:.2f}\".format(_cls, score)\n","        x1, y1, x2, y2 = box\n","        w, h = x2 - x1, y2 - y1\n","        patch = plt.Rectangle(\n","            [x1, y1], w, h, fill=False, edgecolor=color, linewidth=linewidth\n","        )\n","        ax.add_patch(patch)\n","        ax.text(\n","            x1,\n","            y1,\n","            text,\n","            bbox={\"facecolor\": color, \"alpha\": 0.4},\n","            clip_box=ax.clipbox,\n","            clip_on=True,\n","        )\n","    plt.show()\n","    return ax"],"outputs":[],"metadata":{"id":"9eYNoWGNNQp_"}},{"cell_type":"markdown","source":["    - validation data에서 4개의 batch를 가져와서 각 batch의 첫번째 image에 대한 detection 결과를 다음과 같이 화면에 출력하여 확인합니다."],"metadata":{"id":"jQaIZf5EZVOT"}},{"cell_type":"code","execution_count":null,"source":["def prepare_image(image):\n","    image, _, ratio = resize_and_pad_image(image)\n","    image = tf.keras.applications.resnet.preprocess_input(image)\n","    return tf.expand_dims(image, axis=0), ratio\n","\n","\n","val_dataset = tfds.load(\"coco/2017\", split=\"validation\", data_dir=\"data\")\n","int2str = dataset_info.features[\"objects\"][\"label\"].int2str\n","\n","for sample in val_dataset.take(4):\n","    image = tf.cast(sample[\"image\"], dtype=tf.float32)\n","    input_image, ratio = prepare_image(image)\n","    detections = inference_model.predict(input_image)\n","    num_detections = detections.valid_detections[0]\n","    class_names = [\n","        int2str(int(x)) for x in detections.nmsed_classes[0][:num_detections]\n","    ]\n","    visualize_detections(\n","        image,\n","        detections.nmsed_boxes[0][:num_detections] / ratio,\n","        class_names,\n","        detections.nmsed_scores[0][:num_detections],\n","    )"],"outputs":[],"metadata":{"id":"n2c8shVCNFKu"}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{"id":"JT_lOk5DO5NS"}}]}