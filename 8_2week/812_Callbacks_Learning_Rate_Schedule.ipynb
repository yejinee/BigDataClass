{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras import datasets \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "dropout_rate = 0.5\n",
    "\n",
    "input_shape = (32, 32, 3)\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(input_shape)\n",
    "\n",
    "net = layers.Conv2D(32, (3, 3), padding='SAME')(inputs)\n",
    "net = layers.Activation('relu')(net)\n",
    "net = layers.Conv2D(32, (3, 3), padding='SAME')(net)\n",
    "net = layers.Activation('relu')(net)\n",
    "net = layers.MaxPooling2D(pool_size=(2, 2))(net)\n",
    "net = layers.Dropout(dropout_rate)(net)\n",
    "\n",
    "net = layers.Conv2D(64, (3, 3), padding='SAME')(net)\n",
    "net = layers.Activation('relu')(net)\n",
    "net = layers.Conv2D(64, (3, 3), padding='SAME')(net)\n",
    "net = layers.Activation('relu')(net)\n",
    "net = layers.MaxPooling2D(pool_size=(2, 2))(net)\n",
    "net = layers.Dropout(dropout_rate)(net)\n",
    "\n",
    "net = layers.Flatten()(net)\n",
    "net = layers.Dense(512)(net)\n",
    "net = layers.Activation('relu')(net)\n",
    "net = layers.Dropout(dropout_rate)(net)\n",
    "net = layers.Dense(num_classes)(net)\n",
    "net = layers.Activation('softmax')(net)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=net, name='Basic_CNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model is the full model w/o custom layers\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),  # Optimization\n",
    "              loss='sparse_categorical_crossentropy',  # Loss Function \n",
    "              metrics=['accuracy'])  # Metrics / Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = glob('dataset/cifar/train/*.png')[:100]\n",
    "test_paths = glob('dataset/cifar/test/*.png')[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_name(path):\n",
    "    return path.split('_')[-1].replace('.png', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [get_class_name(path) for path in train_paths]\n",
    "class_names = np.unique(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(path):\n",
    "    fname = tf.strings.split(path, '_')[-1]\n",
    "    lbl_name = tf.strings.regex_replace(fname, '.png', '')\n",
    "    onehot = tf.cast(lbl_name == class_names, tf.uint8)\n",
    "    return tf.argmax(onehot)  # 이번에는 onehot이 아닌 label 번호로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_label(path):\n",
    "    gfile = tf.io.read_file(path)\n",
    "    image = tf.io.decode_image(gfile)\n",
    "    image = tf.cast(image, tf.float32) / 255.  # rescale\n",
    "    \n",
    "    label = get_label(path)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocess(image, label):\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function get_label at 0x000001FC2D72C9D0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function get_label at 0x000001FC2D72C9D0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_paths)\n",
    "train_dataset = train_dataset.map(load_image_label, num_parallel_calls=AUTOTUNE)\n",
    "train_dataset = train_dataset.map(image_preprocess, num_parallel_calls=AUTOTUNE)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(train_paths))\n",
    "train_dataset = train_dataset.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_paths)\n",
    "test_dataset = test_dataset.map(load_image_label, num_parallel_calls=AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 콜백 객체 생성 및 scheduler 함수 적용\n",
    "def scheduler(epoch, lr):\n",
    "  tf.print(f'learning_rate: {lr:.5f}')\n",
    "  # 첫 5 에포크 동안 유지\n",
    "  if epoch < 5:\n",
    "    return lr\n",
    "  else:\n",
    "    # 학습률 감소 적용\n",
    "    return lr * tf.math.exp(-0.1)\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-5fd0f0a52bca>:4: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "learning_rate: 0.00100\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 0s 145ms/step - loss: 2.3512 - accuracy: 0.0735 - val_loss: 2.3480 - val_accuracy: 0.0729\n",
      "learning_rate: 0.00100\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 2.3371 - accuracy: 0.1324 - val_loss: 2.3326 - val_accuracy: 0.0729\n",
      "learning_rate: 0.00100\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 147ms/step - loss: 2.3767 - accuracy: 0.0729 - val_loss: 2.3087 - val_accuracy: 0.1771\n",
      "learning_rate: 0.00100\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 2.2853 - accuracy: 0.1324 - val_loss: 2.2960 - val_accuracy: 0.1771\n",
      "learning_rate: 0.00100\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 2.2676 - accuracy: 0.1765 - val_loss: 2.3015 - val_accuracy: 0.1771\n",
      "learning_rate: 0.00100\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 103ms/step - loss: 2.2903 - accuracy: 0.1765 - val_loss: 2.3111 - val_accuracy: 0.1771\n",
      "learning_rate: 0.00090\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 2.3632 - accuracy: 0.0625 - val_loss: 2.3223 - val_accuracy: 0.1771\n",
      "learning_rate: 0.00082\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 2.3133 - accuracy: 0.1471 - val_loss: 2.3371 - val_accuracy: 0.0521\n",
      "learning_rate: 0.00074\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 2.2719 - accuracy: 0.1618 - val_loss: 2.3480 - val_accuracy: 0.0521\n",
      "learning_rate: 0.00067\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 1s 172ms/step - loss: 2.3969 - accuracy: 0.0417 - val_loss: 2.3370 - val_accuracy: 0.0625\n",
      "learning_rate: 0.00061\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 2.2445 - accuracy: 0.0882 - val_loss: 2.3362 - val_accuracy: 0.0729\n",
      "learning_rate: 0.00055\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 2.3148 - accuracy: 0.1176 - val_loss: 2.3337 - val_accuracy: 0.0729\n",
      "learning_rate: 0.00050\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 2.3118 - accuracy: 0.1029 - val_loss: 2.3477 - val_accuracy: 0.0729\n",
      "learning_rate: 0.00045\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 2.2830 - accuracy: 0.1176 - val_loss: 2.3546 - val_accuracy: 0.0729\n",
      "learning_rate: 0.00041\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 150ms/step - loss: 2.3977 - accuracy: 0.0521 - val_loss: 2.3578 - val_accuracy: 0.0729\n",
      "learning_rate: 0.00037\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 2.2710 - accuracy: 0.1029 - val_loss: 2.3604 - val_accuracy: 0.0729\n",
      "learning_rate: 0.00033\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 2.2969 - accuracy: 0.1176 - val_loss: 2.3730 - val_accuracy: 0.0729\n",
      "learning_rate: 0.00030\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 2.3768 - accuracy: 0.0729 - val_loss: 2.3663 - val_accuracy: 0.0729\n",
      "learning_rate: 0.00027\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 2.2691 - accuracy: 0.1176 - val_loss: 2.3673 - val_accuracy: 0.0729\n",
      "learning_rate: 0.00025\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 2.3880 - accuracy: 0.0735 - val_loss: 2.3705 - val_accuracy: 0.0729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fc238dd9d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch = len(train_paths) // batch_size\n",
    "validation_steps = len(test_paths) // batch_size\n",
    "\n",
    "model.fit_generator(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=test_dataset,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=num_epochs,\n",
    "    callbacks=[lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
